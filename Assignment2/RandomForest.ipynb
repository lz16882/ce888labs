{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Concatenate all the training data as another text file: output_train.txt\n",
    "filenames_tr = ['qa1_single-supporting-fact_train.txt', 'qa2_two-supporting-facts_train.txt','qa3_three-supporting-facts_train.txt'\n",
    "            ,'qa4_two-arg-relations_train.txt','qa5_three-arg-relations_train.txt','qa6_yes-no-questions_train.txt',\n",
    "            'qa7_counting_train.txt','qa8_lists-sets_train.txt','qa9_simple-negation_train.txt',\n",
    "            'qa10_indefinite-knowledge_train.txt','qa11_basic-coreference_train.txt','qa12_conjunction_train.txt',\n",
    "            'qa13_compound-coreference_train.txt','qa14_time-reasoning_train.txt','qa15_basic-deduction_train.txt',\n",
    "            'qa16_basic-induction_train.txt','qa17_positional-reasoning_train.txt','qa18_size-reasoning_train.txt',\n",
    "            'qa19_path-finding_train.txt','qa20_agents-motivations_train.txt']\n",
    "with open('output_train.txt', 'w') as outfile:\n",
    "    for fname in filenames_tr:\n",
    "        with open(fname) as infile:\n",
    "            for line in infile:\n",
    "                outfile.write(line)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Concatenate all the testing data as another text file: output_test.txt\n",
    "filenames_te = ['qa1_single-supporting-fact_test.txt', 'qa2_two-supporting-facts_test.txt','qa3_three-supporting-facts_test.txt'\n",
    "            ,'qa4_two-arg-relations_test.txt','qa5_three-arg-relations_test.txt','qa6_yes-no-questions_test.txt',\n",
    "            'qa7_counting_test.txt','qa8_lists-sets_test.txt','qa9_simple-negation_test.txt',\n",
    "            'qa10_indefinite-knowledge_test.txt','qa11_basic-coreference_test.txt','qa12_conjunction_test.txt',\n",
    "            'qa13_compound-coreference_test.txt','qa14_time-reasoning_test.txt','qa15_basic-deduction_test.txt',\n",
    "            'qa16_basic-induction_test.txt','qa17_positional-reasoning_test.txt','qa18_size-reasoning_test.txt',\n",
    "            'qa19_path-finding_test.txt','qa20_agents-motivations_test.txt']\n",
    "with open('output_test.txt', 'w') as outfile:\n",
    "    for fname in filenames_te:\n",
    "        with open(fname) as infile:\n",
    "            for line in infile:\n",
    "                outfile.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Vocab size: 191 unique words\n",
      "Story max length: 1120 words\n",
      "Query max length: 11 words\n",
      "Number of training stories: 20000\n",
      "Number of test stories: 20000\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "#Import the libries needed in this project including the keras,functools,numpy,re and string\n",
    "\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "import re,string\n",
    "\n",
    "\"\"\"\n",
    "   General description of the functions:\n",
    "   The input text files will be read in get_stories. \n",
    "   In this function, parse_stories is ultilzed to parse texts in every line of the text file.\n",
    "   And stories, questions and answers are divided.\n",
    "   The tokenize function excludes the punctuations.\n",
    "   The parse texts can return a large 3D lists consisted of each story, question and answer.\n",
    "   The vectorize_stories can convert the lists obtained in parse_stories into three matrixes with corresponding numbers.\n",
    "   The three matrixes are story matrix, question matrix and answer matrix\n",
    "\"\"\"\n",
    "\n",
    "def tokenize(sent):\n",
    "    #Return the tokens of a sentence without punctuation.\n",
    "\n",
    "    return [x.strip() for x in re.split('(\\W+)?', sent.translate(None, string.punctuation)) if x.strip()]\n",
    "\n",
    "\n",
    "def parse_stories(lines, only_supporting=False):\n",
    "    #Parse stories provided in the bAbi tasks format\n",
    "    #If only_supporting is true, only the sentences that support the answer are kept.\n",
    "\n",
    "    data = []\n",
    "    story = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        nid, line = line.split(' ', 1)\n",
    "        nid = int(nid)\n",
    "        if nid == 1:\n",
    "            story = []\n",
    "        if '\\t' in line:\n",
    "            # Divide the data into three parts including the stories, questions and answers \n",
    "            q, a, supporting = line.split('\\t')\n",
    "            q = tokenize(q)\n",
    "            a = tokenize(a)\n",
    "            substory = None\n",
    "            # Parse the supporting sentences\n",
    "            if only_supporting:\n",
    "                supporting = map(int, supporting.split())\n",
    "                substory = [story[i - 1] for i in supporting]\n",
    "            else:\n",
    "                substory = [x for x in story if x]\n",
    "            # Put the three types of data into lists respectively\n",
    "            data.append((substory, q, a))\n",
    "            story.append('')\n",
    "        else:\n",
    "            sent = tokenize(line)\n",
    "            story.append(sent)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_stories(f, only_supporting=False, max_length=None):\n",
    "    # Read the file,retrieve the stories,\n",
    "    #and then convert the sentences into a single story.\n",
    "    #If max_length is supplied,\n",
    "    #any stories longer than max_length tokens will be discarded.\n",
    "    \n",
    "    data = parse_stories(f.readlines(), only_supporting=only_supporting)\n",
    "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "    data = [(flatten(story), q, answer) for story, q, answer in data if not max_length or len(flatten(story)) < max_length]\n",
    "    return data\n",
    "\n",
    "\n",
    "def vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\n",
    "    #Covert every word in the targeted data into corresponding numbers\n",
    "    X = []\n",
    "    Xq = []\n",
    "    Y = []\n",
    "    for story, query, answer in data:\n",
    "        x = [word_idx[w] for w in story]\n",
    "        xq = [word_idx[w] for w in query]\n",
    "        y = [word_idx[w] for w in answer]\n",
    "        X.append(x)\n",
    "        Xq.append(xq)\n",
    "        Y.append(y)\n",
    "    return (pad_sequences(X, maxlen=story_maxlen),\n",
    "            pad_sequences(Xq, maxlen=query_maxlen), np.array(Y))\n",
    "\n",
    "\n",
    "# Input the training data files and the testing data files \n",
    "with open('output_train.txt') as inputfile:\n",
    "        train_stories=get_stories(inputfile)\n",
    "\n",
    "with open('output_test.txt') as inputfile:\n",
    "        test_stories=get_stories(inputfile)\n",
    "\n",
    "# Specify the unique words in the data and then caculate the whole word size\n",
    "vocab = set()\n",
    "for story, q, answer in train_stories + test_stories:\n",
    "        vocab |= set(story + q + answer)\n",
    "vocab = sorted(vocab)\n",
    "vocab_size = len(vocab) + 1\n",
    "# Caculate the max length of stories and queries\n",
    "story_maxlen = max(map(len, (x for x, _, _ in train_stories + test_stories)))\n",
    "query_maxlen = max(map(len, (x for _, x, _ in train_stories + test_stories)))\n",
    "\n",
    "# Print the information obtained \n",
    "print('-')\n",
    "print('Vocab size:', vocab_size, 'unique words')\n",
    "print('Story max length:', story_maxlen, 'words')\n",
    "print('Query max length:', query_maxlen, 'words')\n",
    "print('Number of training stories:', len(train_stories))\n",
    "print('Number of test stories:', len(test_stories))\n",
    "print('-')\n",
    "\n",
    "# Function to retrieve the unique words in the data and then put them into a dictionary\n",
    "word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "\n",
    "# Convert the obtained lists of words into array with represented numbers\n",
    "inputs_train, queries_train, answers_train = vectorize_stories(train_stories,\n",
    "                                                               word_idx,\n",
    "                                                               story_maxlen,\n",
    "                                                              query_maxlen)\n",
    "\n",
    "inputs_test, queries_test, answers_test = vectorize_stories(test_stories,\n",
    "                                                            word_idx,\n",
    "                                                            story_maxlen,\n",
    "                                                            query_maxlen)\n",
    "\n",
    "\n",
    "# Concatenate the story array and question array into one matrix\n",
    "matrix_train = np.concatenate((inputs_train,queries_train), axis =1)\n",
    "matrix_test = np.concatenate((inputs_test,queries_test), axis =1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:6: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final mean accuracy is 0.4214\n"
     ]
    }
   ],
   "source": [
    "# Import randomforest calssifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Input the training samples into the classifier, and then compare them with the testing smaples\n",
    "# Finally obtian the scores which represents the mean accuracy\n",
    "score = RandomForestClassifier(n_estimators=100).fit(matrix_train,answers_train).score(matrix_test,answers_test)\n",
    "print ('The final mean accuracy is', score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
