{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Vocab size: 20 unique words\n",
      "Story max length: 56 words\n",
      "Query max length: 3 words\n",
      "Number of training stories: 1000\n",
      "Number of test stories: 1000\n",
      "-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.381\n",
      "-\n",
      "Vocab size: 34 unique words\n",
      "Story max length: 464 words\n",
      "Query max length: 4 words\n",
      "Number of training stories: 1000\n",
      "Number of test stories: 1000\n",
      "-\n",
      "2 0.196\n",
      "-\n",
      "Vocab size: 35 unique words\n",
      "Story max length: 1120 words\n",
      "Query max length: 7 words\n",
      "Number of training stories: 1000\n",
      "Number of test stories: 1000\n",
      "-\n",
      "3 0.158\n",
      "-\n",
      "Vocab size: 16 unique words\n",
      "Story max length: 14 words\n",
      "Query max length: 6 words\n",
      "Number of training stories: 1000\n",
      "Number of test stories: 1000\n",
      "-\n",
      "4 0.677\n",
      "-\n",
      "Vocab size: 40 unique words\n",
      "Story max length: 525 words\n",
      "Query max length: 7 words\n",
      "Number of training stories: 1000\n",
      "Number of test stories: 1000\n",
      "-\n",
      "5 0.424\n",
      "-\n",
      "Vocab size: 36 unique words\n",
      "Story max length: 130 words\n",
      "Query max length: 5 words\n",
      "Number of training stories: 1000\n",
      "Number of test stories: 1000\n",
      "-\n",
      "6 0.535\n",
      "-\n",
      "Vocab size: 44 unique words\n",
      "Story max length: 182 words\n",
      "Query max length: 6 words\n",
      "Number of training stories: 1000\n",
      "Number of test stories: 1000\n",
      "-\n",
      "7 0.705\n",
      "-\n",
      "Vocab size: 46 unique words\n",
      "Story max length: 300 words\n",
      "Query max length: 4 words\n",
      "Number of training stories: 1000\n",
      "Number of test stories: 1000\n",
      "-\n",
      "8 0.53\n",
      "-\n",
      "Vocab size: 25 unique words\n",
      "Story max length: 64 words\n",
      "Query max length: 5 words\n",
      "Number of training stories: 1000\n",
      "Number of test stories: 1000\n",
      "-\n",
      "9 0.631\n",
      "-\n",
      "Vocab size: 26 unique words\n",
      "Story max length: 76 words\n",
      "Query max length: 5 words\n",
      "Number of training stories: 1000\n",
      "Number of test stories: 1000\n",
      "-\n",
      "10 0.496\n",
      "-\n",
      "Vocab size: 27 unique words\n",
      "Story max length: 66 words\n",
      "Query max length: 3 words\n",
      "Number of training stories: 1000\n",
      "Number of test stories: 1000\n",
      "-\n",
      "11 0.533\n",
      "-\n",
      "Vocab size: 21 unique words\n",
      "Story max length: 76 words\n",
      "Query max length: 3 words\n",
      "Number of training stories: 1000\n",
      "Number of test stories: 1000\n",
      "-\n",
      "12 0.48\n",
      "-\n",
      "Vocab size: 27 unique words\n",
      "Story max length: 75 words\n",
      "Query max length: 3 words\n",
      "Number of training stories: 1000\n",
      "Number of test stories: 1000\n",
      "-\n",
      "13 0.768\n",
      "-\n",
      "Vocab size: 28 unique words\n",
      "Story max length: 100 words\n",
      "Query max length: 6 words\n",
      "Number of training stories: 1000\n",
      "Number of test stories: 1000\n",
      "-\n",
      "14 0.21\n",
      "-\n",
      "Vocab size: 26 unique words\n",
      "Story max length: 36 words\n",
      "Query max length: 5 words\n",
      "Number of training stories: 1000\n",
      "Number of test stories: 1000\n",
      "-\n",
      "15 0.413\n",
      "-\n",
      "Vocab size: 18 unique words\n",
      "Story max length: 32 words\n",
      "Query max length: 4 words\n",
      "Number of training stories: 1000\n",
      "Number of test stories: 1000\n",
      "-\n",
      "16 0.399\n",
      "-\n",
      "Vocab size: 21 unique words\n",
      "Story max length: 22 words\n",
      "Query max length: 11 words\n",
      "Number of training stories: 1000\n",
      "Number of test stories: 1000\n",
      "-\n",
      "17 0.579\n",
      "-\n",
      "Vocab size: 21 unique words\n",
      "Story max length: 115 words\n",
      "Query max length: 9 words\n",
      "Number of training stories: 1000\n",
      "Number of test stories: 1000\n",
      "-\n",
      "18 0.783\n",
      "-\n",
      "Vocab size: 33 unique words\n",
      "Story max length: 35 words\n",
      "Query max length: 10 words\n",
      "Number of training stories: 1000\n",
      "Number of test stories: 1000\n",
      "-\n",
      "19 0.104\n",
      "-\n",
      "Vocab size: 40 unique words\n",
      "Story max length: 57 words\n",
      "Query max length: 7 words\n",
      "Number of training stories: 1000\n",
      "Number of test stories: 1000\n",
      "-\n",
      "20 0.923\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "#Import the libries needed in this project including the keras,functools,numpy,re and string\n",
    "\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "import re,string\n",
    "from glob import glob\n",
    "\n",
    "def tokenize(sent):\n",
    "    #Return the tokens of a sentence without punctuation.\n",
    "\n",
    "    return [x.strip() for x in re.split('(\\W+)?', sent.translate(None, string.punctuation)) if x.strip()]\n",
    "\n",
    "\n",
    "def parse_stories(lines, only_supporting=False):\n",
    "    #Parse stories provided in the bAbi tasks format\n",
    "    #If only_supporting is true, only the sentences that support the answer are kept.\n",
    "\n",
    "    data = []\n",
    "    story = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        nid, line = line.split(' ', 1)\n",
    "        nid = int(nid)\n",
    "        if nid == 1:\n",
    "            story = []\n",
    "        if '\\t' in line:\n",
    "            # Divide the data into three parts including the stories, questions and answers \n",
    "            q, a, supporting = line.split('\\t')\n",
    "            q = tokenize(q)\n",
    "            a = tokenize(a)\n",
    "            substory = None\n",
    "            # Parse the supporting sentences\n",
    "            if only_supporting:\n",
    "                supporting = map(int, supporting.split())\n",
    "                substory = [story[i - 1] for i in supporting]\n",
    "            else:\n",
    "                substory = [x for x in story if x]\n",
    "            # Put the three types of data into lists respectively\n",
    "            data.append((substory, q, a))\n",
    "            story.append('')\n",
    "        else:\n",
    "            sent = tokenize(line)\n",
    "            story.append(sent)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_stories(f, only_supporting=False, max_length=None):\n",
    "    # Read the file,retrieve the stories,\n",
    "    #and then convert the sentences into a single story.\n",
    "    #If max_length is supplied,\n",
    "    #any stories longer than max_length tokens will be discarded.\n",
    "    \n",
    "    data = parse_stories(f.readlines(), only_supporting=only_supporting)\n",
    "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "    data = [(flatten(story), q, answer) for story, q, answer in data if not max_length or len(flatten(story)) < max_length]\n",
    "    return data\n",
    "\n",
    "\n",
    "def vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\n",
    "    #Covert the targeted data into vectors which are represented by unique numbers\n",
    "    X = []\n",
    "    Xq = []\n",
    "    Y = []\n",
    "    for story, query, answer in data:\n",
    "        x = [word_idx[w] for w in story]\n",
    "        xq = [word_idx[w] for w in query]\n",
    "        y = [word_idx[w] for w in answer]\n",
    "        X.append(x)\n",
    "        Xq.append(xq)\n",
    "        Y.append(y)\n",
    "    return (pad_sequences(X, maxlen=story_maxlen),\n",
    "            pad_sequences(Xq, maxlen=query_maxlen), np.array(Y))\n",
    "\n",
    "train_data_path = [glob('qa' + str(t) + '_*_train.txt')[0] for t in xrange(1, 21)]\n",
    "test_data_path = [glob('qa' + str(t) + '_*_test.txt')[0] for t in xrange(1, 21)]\n",
    "\n",
    "for i in range(0,20):\n",
    "# Input the training data files and the testing data files \n",
    "    with open(train_data_path[i]) as inputfile:\n",
    "            train_stories=get_stories(inputfile)\n",
    "\n",
    "    with open(test_data_path[i]) as inputfile:\n",
    "            test_stories=get_stories(inputfile)\n",
    "\n",
    "    # Specify the unique words in the data and then caculate the whole word size\n",
    "    vocab = set()\n",
    "    for story, q, answer in train_stories + test_stories:\n",
    "            vocab |= set(story + q + answer)\n",
    "    vocab = sorted(vocab)\n",
    "    vocab_size = len(vocab) + 1\n",
    "    # Caculate the max length of stories and queries\n",
    "    story_maxlen = max(map(len, (x for x, _, _ in train_stories + test_stories)))\n",
    "    query_maxlen = max(map(len, (x for _, x, _ in train_stories + test_stories)))\n",
    "\n",
    "    # Print the information obtained \n",
    "    print('-')\n",
    "    print('Vocab size:', vocab_size, 'unique words')\n",
    "    print('Story max length:', story_maxlen, 'words')\n",
    "    print('Query max length:', query_maxlen, 'words')\n",
    "    print('Number of training stories:', len(train_stories))\n",
    "    print('Number of test stories:', len(test_stories))\n",
    "    print('-')\n",
    "\n",
    "    # Function to retrieve the unique words in the data and then put them into a dictionary\n",
    "    word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "\n",
    "    # Convert the obtained lists of words into arraies with represented numbers\n",
    "    inputs_train, queries_train, answers_train = vectorize_stories(train_stories,\n",
    "                                                                   word_idx,\n",
    "                                                                   story_maxlen,\n",
    "                                                                  query_maxlen)\n",
    "\n",
    "    inputs_test, queries_test, answers_test = vectorize_stories(test_stories,\n",
    "                                                                word_idx,\n",
    "                                                                story_maxlen,\n",
    "                                                                query_maxlen)\n",
    "\n",
    "\n",
    "    # Concatenate the story array and question array into one matrix\n",
    "    matrix_train = np.concatenate((inputs_train,queries_train), axis =1)\n",
    "    matrix_test = np.concatenate((inputs_test,queries_test), axis =1)\n",
    "    #print (matrix_train)\n",
    "    # Import randomforest calssifier\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Input the training samples into the classifier, and then compare them with the testing smaples\n",
    "# Finally obtian the scores which represents the mean accuracy\n",
    "    score = RandomForestClassifier(n_estimators=100).fit(matrix_train,answers_train).score(matrix_test,answers_test)\n",
    "    print (i+1,score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
